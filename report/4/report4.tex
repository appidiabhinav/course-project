\documentclass{article}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\author{Muhammad Iqbal Tawakal}
\title{Progress report}

\begin{document}

\maketitle

\section{Selective Search Testing with Pre-trained model}
Each of the testing images from PASCAL VOC 2007 dataset (all 4952 images) produces around 200-300 regions. The regions proposal are produced using selective search algorithm \cite{uijlings2013}. The parameter for initial segmentation is $k=200$ and the similarity measures used for hierarchical grouping are color and texture. Each region is then warped to fit the size of the CNN input by simply resize/warp to 227x227. In the R-CNN work \cite{girshick2013}, other strategies are tightest squares with context and without context.

Each region is then having its CNN features computed which produced a feature vectors with 4096 dimensions. Each feature vector is then predicted using SVM with pre-trained model from the training images. The decision results are aggregated using two different strategies. The first strategy is to average the decision value and the second strategy is simply pick the maximum value. This result is coming from $C=0.4$ and $C=0.8$ for average and max strategy, respectively.

The baseline result is obtained by using this parameter: layer number 7 (second-fully connected layer) in the network, number of jittered images 16, and SVM $C$ variable set to 1.6. If this is compared to the paper \cite{alicvpr2014}, this result is definitely better than CNN-SVM but below the CNNaug-SVM.

CNN implementation used are Caffe \cite{jia2013}.

\begin{figure}[h]
	\begin{subfigure}[h]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../plot/person_5_4.png}
	\end{subfigure}
	\begin{subfigure}[h]{0.4\textwidth}
		\includegraphics[width=\textwidth]{../../plot/plant_5_4.png}
	\end{subfigure}
	\caption{Precision-Recall Curve.}
\end{figure}

\begin{table}[h]
\centerline{    \begin{tabular}{l c c c}
        \hline
        Class & Baseline result & average & max \\
        \hline
	aero & 87.5 & 52.2 & 57.8 \\
	bike & 81.0 & 45.7 & 72.7 \\
	bird & 84.4 & 33.3 & 47.2 \\
	boat & 83.7 & 25.2 & 49.0 \\
	bottle & 43.6 & 29.8 & 40.9 \\
	bus & 70.9 & 35.2 & 62.6 \\
	car & 84.1 & 69.6 & 83.2 \\
	cat & 82.8 & 44.8 & 71.9 \\
	chair & 61.3 & 42.0 & 55.2 \\
	cow & 66.2 & 33.0 & 45.68 \\
	table & 66.5 & 29.5 & 59.5 \\
	dog & 79.4 & 49.0 & 65.2 \\
	horse & 84.9 & 20.5 & 42.6 \\
	mbike & 77.1 & 36.0 & 72.4 \\
	person & 91.7 & 77.9 & 90.0 \\
	plant & 54.2 & 20.7 & 34.2 \\
	sheep & 73.3 & 32.4 & 59.7 \\
	sofa & 66.6 & 35.2 & 64.7 \\
	train & 87.4 & 56.6 & 74.6 \\
	tv & 71.4 & 44.2 & 63.8 \\
	\hline
	mAP & 74.9 & 40.7 & 60.6
    \end{tabular}
}
\caption{PASCAL VOC 2007 Average Precision}
\end{table}

As can be seen in the table 1, the average precision is worse for almost all classes, except for the car and person class. I suspect this is because out of all regions only several regions that constitute the correct object, and ultimately many of those others regions contributed negatively to the final prediction. The max strategy works better because I think this it is the output from a big region that almost encompass the whole image.
I'll try to investigate this issue further by looking at the decision value output.

Figure 1 shows the PR curve for two classes, plant which has worst average precision and human which has best average precision. The blue line is the baseline result and the red line is selective search result.

\section{Classification with selective search training model}
The next step is to train the SVM with all regions.
Similar with the testing phase, there are approximately 1.2 million data points. To reduce the memory consumption, hard negative mining strategy is used.

Implementation is still underway...

\bibliographystyle{plain}
\bibliography{../report}

\end{document}

