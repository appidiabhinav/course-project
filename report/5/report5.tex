\documentclass{article}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\author{Muhammad Iqbal Tawakal}
\title{Progress report}

\begin{document}

\maketitle

\section{Classification with selective search training model}
In this phase, we train SVM linear classifier with CNN features extracted from all regions/segments produced by selective search algorithm \cite{uijlings2013}. There are approximately 1.2 million data points coming from 5011 images \cite{pascalvoc2007}, each produce roughly 200-300 regions. To reduce the memory consumption, hard negative mining strategy is used. In the end, the total number of samples used for training is limited to 400,000 samples due to memory constraint.

The positive samples are taken from all segments of all images which have positive label (200-300 images times 200-300 regions so around 40,000 to 70,000 samples). This is, in restrospect, not really correct since many background regions will also be considered as object. Alas, I have already run the training when Hossein corrected me about this. The training time is also takes considerable amount of time and by days 3 of training I was only able to finish training for 8 classes (bike to chair), each with 8 different values of SVM C parameter.

Table \ref{tab:ap} shows the comparison between baseline result, pre-trained model, and selective search model. There are some improvement from testing with pre-trained model, but overall, beside the bottle class, the result is still below the baseline result. The result is aggregated using max strategy. The parameter C for SVM training for baseline, pre-trained model, and selective search model are 1.6, 0.8, and 3.2 respectively.

\begin{table}[h]
\centerline{    \begin{tabular}{l c c c}
        \hline
        Class & Baseline result & pre-trained model & selective search model \\
        \hline
	bike & \textbf{81.0} & 72.7 & 74.3\\
	bird & \textbf{84.4} & 47.2 & 65.5\\
	boat & \textbf{83.7} & 49.0 & 75.2\\
	bottle & 43.6 & 40.9 & \textbf{45.6}\\
	bus & \textbf{70.9} & 62.6 & 69.0\\
	car & \textbf{84.1} & 83.2 & 79.4\\
	cat & \textbf{82.8} & 71.9 & 75.1\\
	chair & \textbf{61.3} & 55.2 & 51.4\\
	\hline
	mAP (for 8 classes) & \textbf{73.9} & 60.4 & 67.0 \\
    \end{tabular}
}
\caption{PASCAL VOC 2007 Average Precision}
\label{tab:ap}
\end{table}

Next step, I'll use the bounding box annotation provided from the training dataset to extract correct positive samples then rerun the entire thing.

\bibliographystyle{plain}
\bibliography{../report}

\end{document}

